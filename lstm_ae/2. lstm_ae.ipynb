{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly-Detection-in-time-series/Anomaly-Detection-in-Time-Series.ipynb\n",
    "\n",
    "https://github.com/Charlie5DH/Anomaly-Detection-in-time-series/blob/master/Anomaly-Detection-in-Time-Series.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 과정\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[['close']])\n",
    "\n",
    "train['close'] = scaler.transform(train[['close']])\n",
    "test['close'] = scaler.transform(test[['close']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LSTM autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = X_train.shape[1]\n",
    "num_features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(timesteps, num_features)),\n",
    "    Dropout(0.2),\n",
    "    RepeatVector(timesteps), # replicates features from outputs (30 times)\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    # Time distributed layer to get an output with right shape\n",
    "    TimeDistributed(Dense(num_features))\n",
    "])\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[es],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(..., input_shape=(...)))\n",
    "model.add(RepeatVector(...))\n",
    "model.add(LSTM(..., return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(...)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/takanyanta/Sparse-LSTM-Autoencoder-Implementation/blob/main/Sparse_LSTM_autoencoder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Flatten, Reshape, Embedding,ActivityRegularization, Lambda, Input,Multiply,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import changefinder\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "from tensorflow.keras import regularizers, models, layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Conv2D, MaxPool2D, Conv1D\n",
    "from tensorflow.python.keras.layers import Layer, InputSpec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial import distance\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def create_dataset(self, X, y, time_steps=1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.Xs, self.ys = [], []\n",
    "        self.time_steps = time_steps\n",
    "        for self.i in range(len(self.X) - self.time_steps):\n",
    "            self.v = self.X.iloc[self.i:(self.i + self.time_steps)].values\n",
    "            self.Xs.append(self.v)        \n",
    "            self.ys.append(self.y.iloc[self.i + self.time_steps])\n",
    "        return np.array(self.Xs), np.array(self.ys)\n",
    "    \n",
    "    def Creating_Models(self, timesteps, num_features, hidden = 10):\n",
    "        self.hidden = hidden\n",
    "        self.timesteps = timesteps\n",
    "        self.num_features = num_features\n",
    "        self.model = Sequential([\n",
    "            #Encoder\n",
    "            LSTM(self.hidden, input_shape=(self.timesteps, self.num_features)),\n",
    "            #Dropout(0.2),\n",
    "            #Decoder\n",
    "            RepeatVector(self.timesteps),\n",
    "            LSTM(self.hidden, return_sequences=True),\n",
    "            #Dropout(0.2),\n",
    "            TimeDistributed(Dense(self.num_features))                 \n",
    "        ])\n",
    "        return self.model\n",
    "    \n",
    "    def plotting_history(self, history):\n",
    "        self.history = history\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Usual_LSTM(X):\n",
    "    hidden = 5\n",
    "    timesteps=X.shape[1]\n",
    "    num_features=X.shape[2]\n",
    "    model = Sequential([\n",
    "        LSTM(hidden, input_shape=(timesteps, num_features)),\n",
    "        RepeatVector(timesteps),\n",
    "        LSTM(hidden, return_sequences=True),\n",
    "        TimeDistributed(Dense(num_features))                 \n",
    "    ])\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "        \"kernel\", shape=[1, self.units],\n",
    "        initializer='uniform', trainable=True,\n",
    "        regularizer = tf.keras.regularizers.l1(0.001)\n",
    "        )\n",
    "    def call(self, input):\n",
    "        output = input*self.kernel\n",
    "        return tf.nn.relu(output) \n",
    "\n",
    "def Sparse_LSTM(X):\n",
    "    hidden = 5\n",
    "    timesteps=X.shape[1]\n",
    "    num_features=X.shape[2]\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(timesteps, num_features)),\n",
    "        MyLayer(timesteps*num_features),\n",
    "        Reshape(target_shape=(timesteps, num_features)),\n",
    "        LSTM(hidden, input_shape=(timesteps, num_features)),\n",
    "        RepeatVector(timesteps),\n",
    "        LSTM(hidden, return_sequences=True),\n",
    "        TimeDistributed(Dense(num_features))    \n",
    "    ])\n",
    "    model.compile( loss=\"mse\", optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_df(1).iloc[:10000]\n",
    "test = create_df(1).iloc[10000:]\n",
    "sc = StandardScaler()\n",
    "train_std = pd.DataFrame(sc.fit_transform(train), columns=train.columns)\n",
    "test_std = pd.DataFrame(sc.transform(test), columns=test.columns)\n",
    "\n",
    "X_train, _ = preprocess().create_dataset(train_std, train_std, time_steps=15)\n",
    "X_test , _ = preprocess().create_dataset(test_std, test_std, time_steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Usual_LSTM(X_train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
